{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Бонусное домашнее задание"]},{"cell_type":"markdown","metadata":{},"source":["## Part 0\n","\n","В данном домашнем задании вам предстоит реализовать СLIP -- self-supervision модель которая выучивает зависимости между картинками и текстов в едином векторном пространстве. Для выполнения этого домашнего задания вам понадобится GPU и несколько дополнительных библиотек. Автор рекомендует делать все исключительно в Kaggle. \n","\n","\n","[Ссылка на датасет](https://www.kaggle.com/datasets/keenwarrior/small-flicker-data-for-image-captioning)\n","\n","[Ссылка на статью](https://openai.com/research/clip)\n","\n","Задания в ноутбуке будут во многом опираться на статью, поэтому рекомендуется ее прочитать перед выполнением."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T10:50:50.313395Z","iopub.status.busy":"2024-03-17T10:50:50.313051Z","iopub.status.idle":"2024-03-17T10:51:15.943132Z","shell.execute_reply":"2024-03-17T10:51:15.942105Z","shell.execute_reply.started":"2024-03-17T10:50:50.313368Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.16)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.2)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.2)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.1)\n","Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.20.3)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.2.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.9.0)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (21.3)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (9.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->timm) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"]}],"source":["!pip install timm\n","!pip install transformers"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:58:41.580290Z","iopub.status.busy":"2024-03-17T12:58:41.579925Z","iopub.status.idle":"2024-03-17T12:58:50.140447Z","shell.execute_reply":"2024-03-17T12:58:50.139486Z","shell.execute_reply.started":"2024-03-17T12:58:41.580260Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import torchvision.transforms as T\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import glob\n","import numpy as np\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","import timm\n","PATH_TO_IMAGES = '/kaggle/input/small-flicker-data-for-image-captioning/flickr1k/captions.csv'"]},{"cell_type":"markdown","metadata":{},"source":["## Part 1 (8 баллов)\n","\n","Для начала нам нужно реализовать составляющие модели: Кодировщик картинок, текста и проектор на какое-то маломерное пространство. В папке с заданием есть соответствующие файлы, заполните пропуски в них опираясь на docstring-и.\n","\n","Разбалловка следующая: \n","\n","Правильно реализованные кодировщики: 2 балла.\n","\n","Правильно реализованный проектор: 2 балла.\n","\n","Правильно реализованный класс СLIP: 4 балла."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:59:22.309917Z","iopub.status.busy":"2024-03-17T12:59:22.309069Z","iopub.status.idle":"2024-03-17T12:59:23.552013Z","shell.execute_reply":"2024-03-17T12:59:23.551232Z","shell.execute_reply.started":"2024-03-17T12:59:22.309879Z"},"trusted":true},"outputs":[],"source":["from shutil import copyfile\n","\n","# copy our file into the working directory (make sure it has .py suffix)\n","copyfile(src = \"/kaggle/input/shw4-torbakhov/CLIPDataset.py\", dst = \"../working/CLIPDataset.py\")\n","copyfile(src = \"/kaggle/input/shw4-torbakhov/ImageEncoder.py\", dst = \"../working/ImageEncoder.py\")\n","copyfile(src = \"/kaggle/input/shw4-torbakhov/ProjectionHead.py\", dst = \"../working/ProjectionHead.py\")\n","copyfile(src = \"/kaggle/input/shw4-torbakhov/TextEncoder.py\", dst = \"../working/TextEncoder.py\")\n","\n","from CLIPDataset import CLIPDataset\n","from ImageEncoder import ImageEncoder\n","from ProjectionHead import ProjectionHead\n","from TextEncoder import TextEncoder"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:43:20.297479Z","iopub.status.busy":"2024-03-17T12:43:20.296934Z","iopub.status.idle":"2024-03-17T12:43:20.303639Z","shell.execute_reply":"2024-03-17T12:43:20.302771Z","shell.execute_reply.started":"2024-03-17T12:43:20.297450Z"},"trusted":true},"outputs":[],"source":["# from .CLIPDataset import CLIPDataset\n","from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n","# from .ImageEncoder import ImageEncoder\n","# from .ProjectionHead import ProjectionHead\n","# from .TextEncoder import TextEncoder"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:43:21.072653Z","iopub.status.busy":"2024-03-17T12:43:21.072300Z","iopub.status.idle":"2024-03-17T12:43:21.084150Z","shell.execute_reply":"2024-03-17T12:43:21.082889Z","shell.execute_reply.started":"2024-03-17T12:43:21.072625Z"},"trusted":true},"outputs":[],"source":["class CLIP(nn.Module):\n","    def __init__(self, image_embedding=2048, text_embedding=768, temp =1.0):\n","        super().__init__()\n","        self.image_encoder = ImageEncoder()\n","        self.text_encoder = TextEncoder()\n","        self.image_projections = ProjectionHead(image_embedding)\n","        self.text_projections = ProjectionHead(text_embedding)\n","        self.temp = temp\n","    def forward(self, batch):\n","        \"\"\"\n","        :batch: dict of images and text\n","        Here is what you should do:\n","        1) extract image and text features from batch\n","        2) project features into projection space (small latent space)\n","        3) compute cosine similarity with temperature this will be your logits\n","        4) compute \"true\" logits (eg. cosine similarity between images and images, text and text)\n","        5) create targets by averaging similarities from step above (do not forget about temperature)\n","        6) compute mean loss (see paper)\n","        7) return loss\n","\n","        Overall: read paper.\n","        \n","        \"\"\"\n","        #1\n","        image_features = self.image_encoder(batch[\"image\"])\n","        text_features = self.text_encoder(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n","        #2\n","        image_embeddings = self.image_projections(image_features)\n","        text_embeddings = self.text_projections(text_features)\n","        #3\n","        images_similarity = image_embeddings @ image_embeddings.T\n","        texts_similarity = text_embeddings @ text_embeddings.T\n","        #4\n","        logits = (text_embeddings @ image_embeddings.T) / self.temp\n","        #5\n","        targets = F.softmax((images_similarity + texts_similarity) / 2 * self.temp, dim=-1)\n","        #6\n","        loss = (CE(logits, targets) + CE(logits.T, targets.T)) / 2.0 \n","        #7\n","        return loss.mean()\n","    \n","\n","def CE(preds, targets):\n","    log_softmax = nn.LogSoftmax(dim=-1)\n","    loss = (-targets * log_softmax(preds)).sum(1)\n","    return loss   "]},{"cell_type":"markdown","metadata":{},"source":["# Part 2. (0 Баллов)\n","\n","Здесь вам нужно вписать правильный путь до csv файла на своей машине и запустить код."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:43:22.629762Z","iopub.status.busy":"2024-03-17T12:43:22.629183Z","iopub.status.idle":"2024-03-17T12:43:22.636400Z","shell.execute_reply":"2024-03-17T12:43:22.635533Z","shell.execute_reply.started":"2024-03-17T12:43:22.629731Z"},"trusted":true},"outputs":[],"source":["def make_train_valid_dfs():\n","    dataframe = pd.read_csv(f\"{PATH_TO_IMAGES}\")\n","    dataframe[\"id\"] = np.array(list(dataframe.index))\n","    max_id = dataframe[\"id\"].max() + 1\n","    image_ids = np.arange(0, max_id)\n","    np.random.seed(42)\n","    valid_ids = np.random.choice(\n","        image_ids, size=int(0.2 * len(image_ids)), replace=False\n","    )\n","    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n","    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n","    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n","    return train_dataframe, valid_dataframe"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:43:23.459145Z","iopub.status.busy":"2024-03-17T12:43:23.458766Z","iopub.status.idle":"2024-03-17T12:43:23.464598Z","shell.execute_reply":"2024-03-17T12:43:23.463626Z","shell.execute_reply.started":"2024-03-17T12:43:23.459116Z"},"trusted":true},"outputs":[],"source":["def build_loaders(dataframe, tokenizer, mode):\n","    dataset = CLIPDataset(\n","        \"/kaggle/input/small-flicker-data-for-image-captioning/flickr1k/images\",\n","        dataframe[\"image\"].values,\n","        dataframe[\"caption\"].values,\n","        tokenizer=tokenizer\n","    )\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=32,\n","        num_workers=1,\n","        shuffle=True if mode == \"train\" else False,\n","    )\n","    return dataloader, dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:43:24.544289Z","iopub.status.busy":"2024-03-17T12:43:24.543927Z","iopub.status.idle":"2024-03-17T12:43:24.551474Z","shell.execute_reply":"2024-03-17T12:43:24.550544Z","shell.execute_reply.started":"2024-03-17T12:43:24.544245Z"},"trusted":true},"outputs":[],"source":["class AvgMeter:\n","    def __init__(self, name=\"CrossEntropyLoss\"):\n","        self.name = name\n","        self.reset()\n","\n","    def reset(self):\n","        self.avg, self.sum, self.count = [0] * 3\n","\n","    def update(self, val, count=1):\n","        self.count += count\n","        self.sum += val * count\n","        self.avg = self.sum / self.count\n","\n","    def __repr__(self):\n","        text = f\"{self.name}: {self.avg:.4f}\"\n","        return text\n","    \n","    def __format__(self, formatspec):\n","        text = f\"{self.name}: {format(self.avg, formatspec)}\"\n","        return text\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group[\"lr\"]"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:43:25.511051Z","iopub.status.busy":"2024-03-17T12:43:25.510686Z","iopub.status.idle":"2024-03-17T12:43:25.520131Z","shell.execute_reply":"2024-03-17T12:43:25.519108Z","shell.execute_reply.started":"2024-03-17T12:43:25.511022Z"},"trusted":true},"outputs":[],"source":["def train(model, train_loader, optimizer, lr_scheduler, step):\n","    loss_meter = AvgMeter()\n","    for batch in tqdm(train_loader, desc=\"Training\", total=len(train_loader)):\n","        batch = {key: value.to(device) for key, value in batch.items() if key != \"caption\"}\n","        loss = model(batch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        if step == \"batch\":\n","            lr_scheduler.step()\n","        loss_meter.update(loss.item(), batch[\"image\"].shape[0])\n","    return loss_meter\n","\n","@torch.no_grad()\n","def validate(model, validation_loader):\n","    loss_meter = AvgMeter()\n","    for batch in tqdm(validation_loader, desc=\"Validating\", total=len(validation_loader)):\n","        batch = {key: value.to(device) for key, value in batch.items() if key != \"caption\"}\n","        loss = model(batch)\n","        loss_meter.update(loss.item(), batch[\"image\"].shape[0])\n","    return loss_meter"]},{"cell_type":"markdown","metadata":{},"source":["## Part 3. (2 балла)\n","\n","За вас написан минимальный код для обучения, если он запускается и модель учится, то за этот пункт вы получите 0.5 балла. Чтобы получить полный балл за задание вам нужно будет провести несколько экспериментов и поподбирать гиперпараметры. Можно начать со статьи."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:43:27.245047Z","iopub.status.busy":"2024-03-17T12:43:27.244681Z","iopub.status.idle":"2024-03-17T12:43:27.286780Z","shell.execute_reply":"2024-03-17T12:43:27.285774Z","shell.execute_reply.started":"2024-03-17T12:43:27.245018Z"},"trusted":true},"outputs":[],"source":["import itertools\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","EPOCH = 10\n","def procedure():\n","    train_df, validation_df = make_train_valid_dfs()\n","    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","    train_loader, _ = build_loaders(train_df, tokenizer, mode=\"train\")\n","    val_loader, _ = build_loaders(validation_df, tokenizer, mode=\"valid\")\n","    model = CLIP().to(device)\n","    params = [{\"params\": model.image_encoder.parameters()}, \n","              {\"params\" : model.text_encoder.parameters()},\n","              {\"params\" : itertools.chain(model.image_projections.parameters(),\n","                                          model.text_projections.parameters())}]\n","    optimizer = torch.optim.Adam(params)\n","    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=1, factor=0.8)\n","    step=\"epoch\"\n","    for epoch in range(EPOCH):\n","        print(f\"Epoch: {epoch}. Train and Validation in progress...\")\n","        model.train()\n","        train_loss = train(model, train_loader, optimizer, lr_scheduler, step)\n","        model.eval()\n","        val_loss = validate(model, val_loader)\n","        \n","        lr_scheduler.step(val_loss.avg)\n","        print(f\"Epoch: {epoch},\", end=\"\\n\")\n","        print(f\"Train loss: {train_loss:0.3f}\", end=\"\\n\")\n","        print(f\"Validation loss: {val_loss:0.3f}\")\n","    return model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:43:28.168579Z","iopub.status.busy":"2024-03-17T12:43:28.168209Z","iopub.status.idle":"2024-03-17T12:49:41.596609Z","shell.execute_reply":"2024-03-17T12:49:41.595503Z","shell.execute_reply.started":"2024-03-17T12:43:28.168551Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4514360170ac4eb7a2ed42e97141224c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6509bc04696945659ca9aca2d8da00d6","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4ffe666b5cf48a1a92a731611f2f1e2","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b58e01ea97d4793bce6b11432426f6c","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12f14091da3b4f279b16ed2ca876d9be","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd02823a5800421c9e52f9e2295d7ce4","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0. Train and Validation in progress...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d85df772251427ab1893594e2813925","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/125 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03109d09b8e049afa3cba789a0b17e02","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/32 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0,\n","Train loss: CrossEntropyLoss: 5.449\n","Validation loss: CrossEntropyLoss: 3.513\n","Epoch: 1. Train and Validation in progress...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72db3a4f87254d11b77f702d698c6638","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/125 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71e9892ec57746e7ac50c8a93873fcf2","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/32 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1,\n","Train loss: CrossEntropyLoss: 3.081\n","Validation loss: CrossEntropyLoss: 2.918\n","Epoch: 2. Train and Validation in progress...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17bbcaefe5ff4c9e80f40ffe2ea20229","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/125 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29b2c435cf3c461c99e87d61dda673f4","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/32 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 2,\n","Train loss: CrossEntropyLoss: 2.473\n","Validation loss: CrossEntropyLoss: 2.435\n","Epoch: 3. Train and Validation in progress...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd860613dd654c9797e3f3ce543f6dbf","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/125 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"499f49184e6a430d85b5973ce4c5f351","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/32 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 3,\n","Train loss: CrossEntropyLoss: 1.898\n","Validation loss: CrossEntropyLoss: 1.917\n","Epoch: 4. Train and Validation in progress...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08e52e0385a64bf69b7a239dc5faa23e","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/125 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd651e284caa48a8b15a411e98e31faf","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/32 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 4,\n","Train loss: CrossEntropyLoss: 1.496\n","Validation loss: CrossEntropyLoss: 1.706\n","Epoch: 5. Train and Validation in progress...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d66c56593b94993b41348ce7870da0c","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/125 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66d3e72bd4f14db385e3906f089a37ae","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/32 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 5,\n","Train loss: CrossEntropyLoss: 1.258\n","Validation loss: CrossEntropyLoss: 1.550\n","Epoch: 6. Train and Validation in progress...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"118c07b55dfe449dafd604a7af1b013e","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/125 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42406e80806b4c22b8f990f756f932f6","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/32 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 6,\n","Train loss: CrossEntropyLoss: 0.988\n","Validation loss: CrossEntropyLoss: 1.658\n","Epoch: 7. Train and Validation in progress...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29dc74f9fdb64f089cb965c61b4bf10c","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/125 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5eda698288b437d9e77d4821884fd0a","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/32 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 7,\n","Train loss: CrossEntropyLoss: 0.846\n","Validation loss: CrossEntropyLoss: 1.465\n","Epoch: 8. Train and Validation in progress...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"519b6d21aa03453493da53ea4b183307","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/125 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6efb7cd0927b4b7ca2df567022ccc89b","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/32 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 8,\n","Train loss: CrossEntropyLoss: 0.715\n","Validation loss: CrossEntropyLoss: 1.648\n","Epoch: 9. Train and Validation in progress...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"405813a887fc42418c6d44e53070a24b","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/125 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4160887057f548c7b0ff47cc87429235","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/32 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 9,\n","Train loss: CrossEntropyLoss: 0.655\n","Validation loss: CrossEntropyLoss: 1.402\n"]}],"source":["model = procedure()"]},{"cell_type":"markdown","metadata":{},"source":["## Part 4 (0 баллов)\n","\n","Просто посмотрим на результаты."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:49:47.439699Z","iopub.status.busy":"2024-03-17T12:49:47.439313Z","iopub.status.idle":"2024-03-17T12:49:47.447238Z","shell.execute_reply":"2024-03-17T12:49:47.446233Z","shell.execute_reply.started":"2024-03-17T12:49:47.439667Z"},"trusted":true},"outputs":[],"source":["@torch.inference_mode()\n","def get_image_embeddings(valid_df, model):\n","    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","    valid_loader, _ = build_loaders(valid_df, tokenizer, mode=\"valid\")\n","    valid_image_embeddings = []\n","    for batch in tqdm(valid_loader, desc=\"Getting embeddings\", total=len(valid_loader)):\n","        batch = {key: value.to(device) for key, value in batch.items() if key != \"caption\"}\n","        image_features = model.image_encoder(batch[\"image\"].permute(0, 3, 1, 2)).to(device)\n","        image_embeddings = model.image_projections(image_features)\n","        valid_image_embeddings.append(image_embeddings)\n","    return torch.cat(valid_image_embeddings)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T12:49:48.255563Z","iopub.status.busy":"2024-03-17T12:49:48.255181Z","iopub.status.idle":"2024-03-17T12:49:48.265703Z","shell.execute_reply":"2024-03-17T12:49:48.264738Z","shell.execute_reply.started":"2024-03-17T12:49:48.255535Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","@torch.inference_mode()\n","def find_match(model, image_embeddings, text, image_filenames, num_examples=4):\n","    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","    text_encoded = tokenizer([text])\n","    batch = {key : torch.tensor(value).to(device) for key, value in text_encoded.items()}\n","    \n","    text_features = model.text_encoder(batch[\"input_ids\"], batch[\"attention_mask\"])\n","    text_embeddings = model.text_projections(text_features)\n","    \n","    norm_image_embeddings = nn.functional.normalize(image_embeddings, p=2, dim=-1)\n","    norm_text_embeddings = nn.functional.normalize(text_embeddings, p=2, dim=-1)\n","    \n","    similarity = norm_text_embeddings @ norm_image_embeddings.T\n","    \n","    ans, ans_index = torch.topk(similarity.squeeze(0), num_examples * 5)\n","    match = [image_filenames[index] for index in ans_index[::5]]\n","    fig, ax = plt.subplots(int(num_examples/2), int(num_examples/2), figsize= (10, 10))\n","    for m, a in zip(match, ax.flatten()):\n","        image = Image.open(f\"{PATH_TO_IMAGES}\" + f\"/{m}\")\n","        image = image.convert(\"RGB\")\n","        a.imshow(image)\n","        a.axis(\"off\")\n","    plt.show()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_, valid_df = make_train_valid_dfs()\n","image_embeddings = get_image_embeddings(valid_df, model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-17T12:49:52.154722Z","iopub.status.idle":"2024-03-17T12:49:52.155084Z","shell.execute_reply":"2024-03-17T12:49:52.154930Z","shell.execute_reply.started":"2024-03-17T12:49:52.154915Z"},"trusted":true},"outputs":[],"source":["find_match(model, image_embeddings, \"dogs\", valid_df[\"image\"].values)"]},{"cell_type":"markdown","metadata":{},"source":["## Опишите свои результаты."]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":633282,"sourceId":1126359,"sourceType":"datasetVersion"},{"datasetId":4615591,"sourceId":7867004,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3.11.1 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"},"vscode":{"interpreter":{"hash":"5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"}}},"nbformat":4,"nbformat_minor":5}
