{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import torch\n","import torchvision.transforms as T\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","from PIL import Image\n","import pandas as pd\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import wandb"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# wandb.login(key='')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.manual_seed(42)\n","\n","def get_datasets(root, load_to_ram=True, train_transform=None, test_transform=None, val_size=0.2):\n","    labels = pd.read_csv('/kaggle/input/hse-dl-bhw-1-dataset/bhw1/labels.csv')\n","    per_category = labels.groupby('Category').agg({'Id':'unique'}).Id.to_numpy()\n","    train_all_files = []\n","    train_all_labels = []\n","    val_all_files = []\n","    val_all_labels = []\n","\n","    for i, files in tqdm(enumerate(per_category), total=len(per_category)):\n","        valid_size = int(val_size * len(files))\n","        train_size = len(files) - valid_size\n","        train_files, val_files = torch.utils.data.random_split(files, [train_size, valid_size])\n","        train_all_files += train_files\n","        train_all_labels += [i] * len(train_files)\n","\n","        val_all_files += val_files\n","        val_all_labels += [i] * len(val_files)\n","        \n","    train_dataset = ImageDataset(root, train_all_files, train_all_labels, load_to_ram=load_to_ram, transform=train_transform)\n","    val_dataset = ImageDataset(root, val_all_files, val_all_labels, load_to_ram=load_to_ram, transform=test_transform)\n","    \n","    return train_dataset, val_dataset\n","    \n","\n","class ImageDataset(Dataset):\n","    NUM_CLASSES = 200\n","    SPLIT_RANDOM_SEED = 42\n","    \n","    def __init__(self, root, all_files, all_labels, load_to_ram=True, transform=None):\n","        super().__init__()\n","        self.root = root\n","        self.load_to_ram = load_to_ram\n","        self.transform = transform\n","        self.all_files = all_files\n","        self.all_labels = all_labels\n","        self.images = []\n","\n","        self.classes = np.arange(self.NUM_CLASSES)\n","        if self.load_to_ram:\n","            self.images += self._load_images(self.all_files)\n","\n","    def _load_images(self, image_files):\n","        images = []\n","        for filename in tqdm(image_files):\n","            image = Image.open(os.path.join(self.root, filename)).convert('RGB')\n","            images += [image]\n","\n","        return images\n","\n","    def __len__(self):\n","        return len(self.all_files)\n","\n","    def __getitem__(self, item):\n","        label = self.all_labels[item]\n","        if self.load_to_ram:\n","            image = self.images[item]\n","        else:\n","            filename = self.all_files[item]\n","            image = Image.open(os.path.join(self.root, filename)).convert('RGB')\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        return image, label\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TestImageDataset(Dataset):\n","    NUM_CLASSES = 200\n","    SPLIT_RANDOM_SEED = 42\n","    \n","    def __init__(self, root, load_to_ram=True, transform=None):\n","        super().__init__()\n","        self.root = root\n","        self.load_to_ram = load_to_ram\n","        self.transform = transform\n","        self.to_tensor = T.ToTensor()\n","        self.all_files = []\n","        self.images = []\n","\n","        self.classes = np.arange(self.NUM_CLASSES)\n","        for file in tqdm(os.listdir(self.root)):\n","            self.all_files += [file]\n","            if self.load_to_ram:\n","                self.images += [Image.open(os.path.join(self.root, file)).convert('RGB')]\n","                \n","    def __len__(self):\n","        return len(self.all_files)\n","\n","    def __getitem__(self, item):\n","        file = self.all_files[item]\n","        if self.load_to_ram:\n","            image = self.images[item]\n","        else:\n","            image = Image.open(os.path.join(self.root, file)).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","train_transform = T.Compose([\n","    T.RandomResizedCrop(40),\n","    T.RandomHorizontalFlip(),\n","    T.AugMix(),\n","    T.ToTensor(),\n","    normalize,\n","])\n","\n","test_transform = T.Compose([\n","    T.ToTensor(),\n","    normalize,\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_dataset, val_dataset = get_datasets(root='/kaggle/input/hse-dl-bhw-1-dataset/bhw1/trainval', load_to_ram=True, train_transform=train_transform, test_transform=test_transform)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, pin_memory=True)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# def log_train(train_loss, train_acc, val_loss, val_acc):   \n","#     wandb.log({\"train_loss\": train_loss, \n","#                    \"train_acc\": train_acc,\n","#                    \"val_loss\": val_loss, \n","#                    \"val_acc\": val_acc})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_accuracy = 0.0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@torch.no_grad()\n","def test(model, loader):\n","    criterion = nn.CrossEntropyLoss()\n","    loss_log = []\n","    acc_log = []\n","    model.eval()\n","    \n","    for data, target in tqdm(loader):\n","        data = data.to(device)\n","        target = target.to(device)\n","        \n","        logits = model(data)\n","        loss = criterion(logits, target)\n","        \n","        loss_log.append(loss.item())\n","        \n","        acc = (logits.argmax(dim=1) == target).sum() / data.shape[0]\n","        \n","        acc_log.append(acc.item()) \n","        \n","    return np.mean(loss_log), np.mean(acc_log)\n","\n","def train_epoch(model, optimizer, train_loader):\n","    criterion = nn.CrossEntropyLoss()\n","    loss_log = []\n","    acc_log = []\n","    model.train()\n","    \n","    for data, target in tqdm(train_loader):\n","        data = data.to(device)\n","        target = target.to(device)\n","        \n","        optimizer.zero_grad()\n","        logits = model(data) \n","        loss = criterion(logits, target)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        loss_log.append(loss.item())\n","        \n","        acc = (logits.argmax(dim=1) == target).sum() / data.shape[0]\n","        \n","        acc_log.append(acc.item()) \n","        \n","\n","    return loss_log, acc_log\n","\n","def train(model, optimizer, n_epochs, train_loader, val_loader, scheduler):\n","    best_accuracy = 0.0\n","    train_loss_log, train_acc_log, val_loss_log, val_acc_log = [], [], [], []\n","\n","    for epoch in range(n_epochs):\n","        train_loss, train_acc = train_epoch(model, optimizer, train_loader)\n","        val_loss, val_acc = test(model, val_loader)\n","        \n","        train_loss_log.extend(train_loss)\n","        train_acc_log.extend(train_acc)\n","        \n","        val_loss_log.append(val_loss)\n","        val_acc_log.append(val_acc)\n","\n","        print(f\"Epoch {epoch}\")\n","        print(f\" train loss: {np.mean(train_loss)}, train acc: {np.mean(train_acc)}\")\n","        print(f\" val loss: {val_loss}, val acc: {val_acc}\\n\")\n","#         log_train(np.mean(train_loss).item(), np.mean(train_acc).item(), val_loss, val_acc)\n","        \n","        if scheduler is not None:\n","            scheduler.step()\n","            \n","        if val_acc > best_accuracy:\n","            best_accuracy = val_acc\n","            torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            }, \"best_model.pt\")\n","\n","    return train_loss_log, train_acc_log, val_loss_log, val_acc_log"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SuperBlockNet(nn.Module):\n","    def _conv_block(self, in_channels, out_channels, kernel_size=3, stride=1, padding='same', bias=False, relu=True):\n","        layers = []\n","        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias))\n","        layers.append(nn.BatchNorm2d(out_channels))\n","        if relu:\n","            layers.append(nn.ReLU())\n","        return nn.Sequential(*layers)\n","\n","    def _space_to_depth_x2(self, x):\n","        batch_size, channels, height, width = x.size()\n","        unfolded_x = F.unfold(x, 2, stride=2)\n","        return unfolded_x.view(batch_size, channels * 4, height // 2, width // 2)\n","    \n","    def __init__(self, num_classes):\n","        super(SuperBlockNet, self).__init__()\n","\n","        self.layer1 = self._conv_block(3, 32)\n","        self.layer2 = self._conv_block(32, 64)\n","        self.layer3 = self._conv_block(64, 128)\n","        self.layer4 = self._conv_block(128, 256)\n","        self.layer5 = self._conv_block(256, 512)\n","        self.layer6 = nn.MaxPool2d(2)\n","\n","        self.layer7 = self._conv_block(512, 64)\n","        self.layer8 = self._conv_block(64, 128)\n","        self.layer9 = self._conv_block(128, 256)\n","        self.layer10 = self._conv_block(256, 512)\n","        self.layer11 = self._conv_block(512, 1024)\n","        self.layer12 = nn.MaxPool2d(2)\n","\n","        self.layer14 = self._conv_block(3072, 32)\n","        self.layer15 = self._conv_block(32, 128)\n","        self.layer16 = self._conv_block(128, 256)\n","        self.layer17 = self._conv_block(256, 512)\n","        self.layer18 = self._conv_block(512, 1024)\n","        self.layer19 = nn.MaxPool2d(2)\n","\n","        self.layer21 = self._conv_block(13312, num_classes, kernel_size=1,relu=False)\n","        self.layer22 = nn.AdaptiveAvgPool2d(1)\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.layer5(x)\n","        x = self.layer6(x)\n","\n","        skip_connection_1 = x\n","\n","        x = self.layer7(x)\n","        x = self.layer8(x)\n","        x = self.layer9(x)\n","        x = self.layer10(x)\n","        x = self.layer11(x)\n","        x = self.layer12(x)\n","\n","        skip_connection_1 = self._space_to_depth_x2(skip_connection_1)\n","\n","        x = torch.cat([x, skip_connection_1], dim=1)\n","\n","        skip_connection_2 = x\n","\n","        x = self.layer14(x)\n","        x = self.layer15(x)\n","        x = self.layer16(x)\n","        x = self.layer17(x)\n","        x = self.layer18(x)\n","        x = self.layer19(x)\n","\n","        skip_connection_2 = self._space_to_depth_x2(skip_connection_2)\n","\n","        x = torch.cat([x, skip_connection_2], dim=1)\n","\n","        x = self.layer21(x)\n","        x = self.layer22(x)\n","        x = x.view(x.size(0), -1)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_classes = 200\n","num_epochs = 45"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# wandb.init(project=\"bhw1-dl\", config={\"dataset\": \"kaggle-dataset\"}, name=\"SuperBlockNet no validation\")\n","# wandb.config.epochs = num_epochs\n","# wandb.config.optimizer = \"SGD + momentum\"\n","# wandb.config.criterion = \"CrossEntropyLoss\"\n","# wandb.config.scheduler = \"Linear + CosineAnnealingLR\"\n","# wandb.config.learning_rate = \"0.1\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["start_epochs = 5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = SuperBlockNet(n_classes).to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=start_epochs)\n","scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs - start_epochs, eta_min=0)\n","scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[start_epochs])\n","train_loss_log, train_acc_log, val_loss_log, val_acc_log = train(\n","    model, optimizer, num_epochs, train_loader, val_loader, scheduler\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict(model, test_loader):\n","    model.eval()\n","    predictions = []\n","    \n","    for images, files in tqdm(test_loader):\n","        data = images.to(device)\n","        logits = model(data)\n","        preds = logits.argmax(dim=1)\n","        \n","        for file, pred in zip(files, preds):\n","            predictions.append([file, pred.item()])\n","    \n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_dataset = TestImageDataset(root='/kaggle/input/hse-dl-bhw-1-dataset/bhw1/test', load_to_ram=True, transform=test_transform)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = predict(model, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.DataFrame(predictions, columns=['Id', 'Category']).sort_values('Id').to_csv('labels_test.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4212322,"sourceId":7267064,"sourceType":"datasetVersion"},{"datasetId":4328257,"sourceId":7436798,"sourceType":"datasetVersion"},{"datasetId":4329855,"sourceId":7439393,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3.11.1 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"},"vscode":{"interpreter":{"hash":"5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"}}},"nbformat":4,"nbformat_minor":4}
